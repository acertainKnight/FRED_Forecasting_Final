{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step lstm\n",
    "# %pip install chart_studio --user\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import ZeroPadding1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Reshape\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio.plotly as py\n",
    "import chart_studio\n",
    "\n",
    "# for changing the plot size in the Jupyter Notebook output\n",
    "%matplotlib inline\n",
    "# sets the plot size to 12x8\n",
    "mpl.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>W875RX1</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <th>RETAILx</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>DARfor_Step1</th>\n",
       "      <th>DARfor_Step2</th>\n",
       "      <th>DARfor_Step3</th>\n",
       "      <th>DARfor_Step4</th>\n",
       "      <th>DARfor_Step5</th>\n",
       "      <th>DARMfor_Step1</th>\n",
       "      <th>DARMfor_Step2</th>\n",
       "      <th>DARMfor_Step3</th>\n",
       "      <th>DARMfor_Step4</th>\n",
       "      <th>DARMfor_Step5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959-03-01</th>\n",
       "      <td>2462.689</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>17.647</td>\n",
       "      <td>2.934254e+05</td>\n",
       "      <td>18523.05762</td>\n",
       "      <td>23.4004</td>\n",
       "      <td>23.9186</td>\n",
       "      <td>22.4925</td>\n",
       "      <td>32.6455</td>\n",
       "      <td>22.5365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-04-01</th>\n",
       "      <td>2478.744</td>\n",
       "      <td>2330.3</td>\n",
       "      <td>17.584</td>\n",
       "      <td>2.993317e+05</td>\n",
       "      <td>18534.46600</td>\n",
       "      <td>23.8989</td>\n",
       "      <td>24.2641</td>\n",
       "      <td>22.8221</td>\n",
       "      <td>33.1606</td>\n",
       "      <td>22.6807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-05-01</th>\n",
       "      <td>2493.228</td>\n",
       "      <td>2345.8</td>\n",
       "      <td>17.796</td>\n",
       "      <td>3.013730e+05</td>\n",
       "      <td>18679.66354</td>\n",
       "      <td>24.2589</td>\n",
       "      <td>24.4655</td>\n",
       "      <td>23.0418</td>\n",
       "      <td>33.3190</td>\n",
       "      <td>23.1424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-06-01</th>\n",
       "      <td>2500.989</td>\n",
       "      <td>2352.9</td>\n",
       "      <td>17.861</td>\n",
       "      <td>3.013648e+05</td>\n",
       "      <td>18849.75209</td>\n",
       "      <td>24.2866</td>\n",
       "      <td>24.6382</td>\n",
       "      <td>23.2066</td>\n",
       "      <td>33.1606</td>\n",
       "      <td>23.3156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-07-01</th>\n",
       "      <td>2499.525</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>17.801</td>\n",
       "      <td>3.050348e+05</td>\n",
       "      <td>18843.52934</td>\n",
       "      <td>23.7050</td>\n",
       "      <td>24.6670</td>\n",
       "      <td>23.3988</td>\n",
       "      <td>33.5964</td>\n",
       "      <td>23.7773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>16979.701</td>\n",
       "      <td>14096.2</td>\n",
       "      <td>120.433</td>\n",
       "      <td>1.503347e+06</td>\n",
       "      <td>518131.00000</td>\n",
       "      <td>109.2508</td>\n",
       "      <td>104.3141</td>\n",
       "      <td>103.0011</td>\n",
       "      <td>105.0774</td>\n",
       "      <td>120.4686</td>\n",
       "      <td>...</td>\n",
       "      <td>3.036702</td>\n",
       "      <td>2.923689</td>\n",
       "      <td>2.905251</td>\n",
       "      <td>2.738333</td>\n",
       "      <td>2.423327</td>\n",
       "      <td>2.680101</td>\n",
       "      <td>3.069954</td>\n",
       "      <td>2.978124</td>\n",
       "      <td>2.969964</td>\n",
       "      <td>2.659986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>17032.129</td>\n",
       "      <td>14138.7</td>\n",
       "      <td>120.683</td>\n",
       "      <td>1.515264e+06</td>\n",
       "      <td>520055.00000</td>\n",
       "      <td>109.3078</td>\n",
       "      <td>104.6945</td>\n",
       "      <td>103.5907</td>\n",
       "      <td>105.6160</td>\n",
       "      <td>122.1042</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551368</td>\n",
       "      <td>3.007423</td>\n",
       "      <td>2.915597</td>\n",
       "      <td>2.886860</td>\n",
       "      <td>2.733533</td>\n",
       "      <td>2.641499</td>\n",
       "      <td>2.775589</td>\n",
       "      <td>3.007836</td>\n",
       "      <td>2.962984</td>\n",
       "      <td>3.038477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>17009.545</td>\n",
       "      <td>14113.3</td>\n",
       "      <td>121.004</td>\n",
       "      <td>1.514859e+06</td>\n",
       "      <td>523922.00000</td>\n",
       "      <td>109.1045</td>\n",
       "      <td>104.5174</td>\n",
       "      <td>103.4120</td>\n",
       "      <td>105.6055</td>\n",
       "      <td>122.6444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551368</td>\n",
       "      <td>3.007423</td>\n",
       "      <td>2.915597</td>\n",
       "      <td>2.886860</td>\n",
       "      <td>2.733533</td>\n",
       "      <td>2.641499</td>\n",
       "      <td>2.775589</td>\n",
       "      <td>3.007836</td>\n",
       "      <td>2.962984</td>\n",
       "      <td>3.038477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>17085.082</td>\n",
       "      <td>14179.9</td>\n",
       "      <td>121.241</td>\n",
       "      <td>1.525005e+06</td>\n",
       "      <td>526889.00000</td>\n",
       "      <td>109.9431</td>\n",
       "      <td>104.9125</td>\n",
       "      <td>103.7175</td>\n",
       "      <td>105.7062</td>\n",
       "      <td>122.3672</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551368</td>\n",
       "      <td>3.007423</td>\n",
       "      <td>2.915597</td>\n",
       "      <td>2.886860</td>\n",
       "      <td>2.733533</td>\n",
       "      <td>2.641499</td>\n",
       "      <td>2.775589</td>\n",
       "      <td>3.007836</td>\n",
       "      <td>2.962984</td>\n",
       "      <td>3.038477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>17131.657</td>\n",
       "      <td>14216.5</td>\n",
       "      <td>121.448</td>\n",
       "      <td>1.525005e+06</td>\n",
       "      <td>525560.00000</td>\n",
       "      <td>109.5174</td>\n",
       "      <td>104.5979</td>\n",
       "      <td>103.3590</td>\n",
       "      <td>105.4467</td>\n",
       "      <td>119.9819</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551368</td>\n",
       "      <td>3.007423</td>\n",
       "      <td>2.915597</td>\n",
       "      <td>2.886860</td>\n",
       "      <td>2.733533</td>\n",
       "      <td>2.641499</td>\n",
       "      <td>2.775589</td>\n",
       "      <td>3.007836</td>\n",
       "      <td>2.962984</td>\n",
       "      <td>3.038477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  RPI  W875RX1  DPCERA3M086SBEA     CMRMTSPLx       RETAILx  \\\n",
       "sasdate                                                                       \n",
       "1959-03-01   2462.689   2314.0           17.647  2.934254e+05   18523.05762   \n",
       "1959-04-01   2478.744   2330.3           17.584  2.993317e+05   18534.46600   \n",
       "1959-05-01   2493.228   2345.8           17.796  3.013730e+05   18679.66354   \n",
       "1959-06-01   2500.989   2352.9           17.861  3.013648e+05   18849.75209   \n",
       "1959-07-01   2499.525   2351.0           17.801  3.050348e+05   18843.52934   \n",
       "...               ...      ...              ...           ...           ...   \n",
       "2019-05-01  16979.701  14096.2          120.433  1.503347e+06  518131.00000   \n",
       "2019-06-01  17032.129  14138.7          120.683  1.515264e+06  520055.00000   \n",
       "2019-07-01  17009.545  14113.3          121.004  1.514859e+06  523922.00000   \n",
       "2019-08-01  17085.082  14179.9          121.241  1.525005e+06  526889.00000   \n",
       "2019-09-01  17131.657  14216.5          121.448  1.525005e+06  525560.00000   \n",
       "\n",
       "              INDPRO   IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  ...  \\\n",
       "sasdate                                                       ...   \n",
       "1959-03-01   23.4004   23.9186   22.4925   32.6455   22.5365  ...   \n",
       "1959-04-01   23.8989   24.2641   22.8221   33.1606   22.6807  ...   \n",
       "1959-05-01   24.2589   24.4655   23.0418   33.3190   23.1424  ...   \n",
       "1959-06-01   24.2866   24.6382   23.2066   33.1606   23.3156  ...   \n",
       "1959-07-01   23.7050   24.6670   23.3988   33.5964   23.7773  ...   \n",
       "...              ...       ...       ...       ...       ...  ...   \n",
       "2019-05-01  109.2508  104.3141  103.0011  105.0774  120.4686  ...   \n",
       "2019-06-01  109.3078  104.6945  103.5907  105.6160  122.1042  ...   \n",
       "2019-07-01  109.1045  104.5174  103.4120  105.6055  122.6444  ...   \n",
       "2019-08-01  109.9431  104.9125  103.7175  105.7062  122.3672  ...   \n",
       "2019-09-01  109.5174  104.5979  103.3590  105.4467  119.9819  ...   \n",
       "\n",
       "            DARfor_Step1  DARfor_Step2  DARfor_Step3  DARfor_Step4  \\\n",
       "sasdate                                                              \n",
       "1959-03-01      0.000000      0.000000      0.000000      0.000000   \n",
       "1959-04-01      0.000000      0.000000      0.000000      0.000000   \n",
       "1959-05-01      0.000000      0.000000      0.000000      0.000000   \n",
       "1959-06-01      0.000000      0.000000      0.000000      0.000000   \n",
       "1959-07-01      0.000000      0.000000      0.000000      0.000000   \n",
       "...                  ...           ...           ...           ...   \n",
       "2019-05-01      3.036702      2.923689      2.905251      2.738333   \n",
       "2019-06-01      2.551368      3.007423      2.915597      2.886860   \n",
       "2019-07-01      2.551368      3.007423      2.915597      2.886860   \n",
       "2019-08-01      2.551368      3.007423      2.915597      2.886860   \n",
       "2019-09-01      2.551368      3.007423      2.915597      2.886860   \n",
       "\n",
       "            DARfor_Step5  DARMfor_Step1  DARMfor_Step2  DARMfor_Step3  \\\n",
       "sasdate                                                                 \n",
       "1959-03-01      0.000000       0.000000       0.000000       0.000000   \n",
       "1959-04-01      0.000000       0.000000       0.000000       0.000000   \n",
       "1959-05-01      0.000000       0.000000       0.000000       0.000000   \n",
       "1959-06-01      0.000000       0.000000       0.000000       0.000000   \n",
       "1959-07-01      0.000000       0.000000       0.000000       0.000000   \n",
       "...                  ...            ...            ...            ...   \n",
       "2019-05-01      2.423327       2.680101       3.069954       2.978124   \n",
       "2019-06-01      2.733533       2.641499       2.775589       3.007836   \n",
       "2019-07-01      2.733533       2.641499       2.775589       3.007836   \n",
       "2019-08-01      2.733533       2.641499       2.775589       3.007836   \n",
       "2019-09-01      2.733533       2.641499       2.775589       3.007836   \n",
       "\n",
       "            DARMfor_Step4  DARMfor_Step5  \n",
       "sasdate                                   \n",
       "1959-03-01       0.000000       0.000000  \n",
       "1959-04-01       0.000000       0.000000  \n",
       "1959-05-01       0.000000       0.000000  \n",
       "1959-06-01       0.000000       0.000000  \n",
       "1959-07-01       0.000000       0.000000  \n",
       "...                   ...            ...  \n",
       "2019-05-01       2.969964       2.659986  \n",
       "2019-06-01       2.962984       3.038477  \n",
       "2019-07-01       2.962984       3.038477  \n",
       "2019-08-01       2.962984       3.038477  \n",
       "2019-09-01       2.962984       3.038477  \n",
       "\n",
       "[727 rows x 153 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qd = pd.read_csv(r'FRED_QD_20191113.csv', header=0, skiprows=[1,2])\n",
    "md = pd.read_csv(r'FRED_MD_20191113.csv', header=0, skiprows=[1,2])\n",
    "data = md\n",
    "# data['CPI'] = ((data['CPIAUCSL'].shift(-12) / data['CPIAUCSL']) - 1)*100\n",
    "# data = md.merge(qd, how='left', on='sasdate')\n",
    "# data['GDP_pad'] = data.GDPC1.fillna(method='pad')\n",
    "# data['GDP_pad_shift'] = data['GDP_pad'].shift(-3)\n",
    "# data['GDP_change'] = (data['GDP_pad_shift'] - data['GDP_pad'])/3\n",
    "# data['GDP'] = data['GDP_change'].cumsum() + data['GDPC1'][1]\n",
    "data['sasdate'] = pd.to_datetime(data['sasdate'], format=\"%m/%d/%Y\")\n",
    "data.set_index('sasdate', inplace=True)\n",
    "# data = data.loc[:, ~data.columns.str.contains('_y')]\n",
    "# data = data.drop(['GDP_pad', 'GDP_pad_shift', 'GDP_change'], axis=1)\n",
    "data = data.fillna(method='pad').fillna(0).iloc[1:,:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_1(data, train_share, idx=None):\n",
    "    '''split a univariate dataset into train/test sets'''\n",
    "    if idx is not None:\n",
    "        index = idx\n",
    "    else: \n",
    "        index = round(len(data)*train_share)\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "    train, test = data[:index], data[index:]\n",
    "    return train, test\n",
    "    # restructure into windows of yearly data\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences_multi(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[i:end_ix]\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    " \n",
    "    \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    " \n",
    "    \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    yhat_list = []\n",
    "    j = 0\n",
    "    for y in yhat:\n",
    "        yhat_list.append(y + history[-(interval-j)])\n",
    "        j += 1\n",
    "    return yhat_list\n",
    " \n",
    "    \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    " \n",
    "    \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value, n_out):\n",
    "#     print(X)\n",
    "#     print(value)\n",
    "    new_row = [x for x in X] + [y for y in value[0]]\n",
    "#     print(new_row)\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -n_out:]\n",
    "\n",
    "\n",
    "def moving(x, window):\n",
    "    ret = np.cumsum(x, dtype=float)\n",
    "    ret[window:] = ret[window:] - ret[:-window]\n",
    "    return ret[window - 1:] / window\n",
    "\n",
    "\n",
    "def moving_frame(x, window_list=[3]):\n",
    "    frame = pd.DataFrame()\n",
    "    for i in window_list:\n",
    "        frame['{}'.format(i)] = np.pad(moving(x, i), (i-1,0),'constant', constant_values=(0))\n",
    "    return frame.values\n",
    "        \n",
    "\n",
    "def model_branch(input_shape, filter_size, dense_neuron=100, nb_filters=10, drop=.3, batch_size=1):\n",
    "    #the input is a time series of length n and width 19\n",
    "    input_seq = Input(batch_shape=(batch_size, input_shape[1], input_shape[2]))\n",
    "    #1-D convolution and global max-pooling\n",
    "    conv = Conv1D(nb_filters, filter_size, padding=\"same\", activation=\"relu\")(input_seq)\n",
    "#     pool = GlobalMaxPooling1D()(conv)\n",
    "    #dense layer with dropout regularization\n",
    "#     flat = Flatten()(pool)\n",
    "#     compressed = Dense(dense_neuron, activation=\"relu\")(flat)\n",
    "#     compressed = Dropout(drop)(compressed)\n",
    "    model = Model(inputs=input_seq, outputs=conv)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_cnn(merged_shape, nb_filters=10, filter_size=8):\n",
    "    input_seq = Input(shape=(merged_shape))\n",
    "    conv1 = Conv1D(nb_filters, filter_size, padding=\"same\", activation=\"relu\")(input_seq)\n",
    "    pool1 = GlobalMaxPooling1D()(conv1)\n",
    "    model = Model(inputs=input_seq, outputs=pool1)\n",
    "    return model\n",
    "\n",
    "\n",
    "# fit an CONV network to training data\n",
    "def fit_convlstm(train, validate=False, batch_size=1, nb_epoch=1000, neurons=(100,100,100), filter_size=8, \n",
    "                 drop=.5, loss='mean_absolute_error', verbose=0, iteration=0, optimizer='sgd', \n",
    "                 callback=False, window=24, stepsout=12, l_rate=.0001, decay=0, patience=100, \n",
    "                 average_window=[3, 6, 12]):\n",
    "\n",
    "    if validate:\n",
    "        orig, y_train= split_sequence(train, window, stepsout)\n",
    "        moving_average = split_sequences_multi(moving_frame(train, average_window), window, stepsout)[:-1]\n",
    "        orig_val = orig[::10]\n",
    "        orig_val = orig_val.reshape(orig_val.shape[0], orig_val.shape[1], 1)\n",
    "        moving_average_val = moving_average[::10]\n",
    "        moving_average_val = moving_average_val.reshape(moving_average_val.shape[0], moving_average_val.shape[1], \n",
    "                                                        moving_average_val.shape[2])\n",
    "        orig = orig[np.mod(np.arange(orig.shape[0]),10)!=0]\n",
    "        orig = orig.reshape(orig.shape[0], orig.shape[1], 1)\n",
    "        moving_average = moving_average[np.mod(np.arange(moving_average.shape[0]),10)!=0]\n",
    "        moving_average = moving_average.reshape(moving_average.shape[0], moving_average.shape[1], \n",
    "                                                moving_average.shape[2])\n",
    "        y_val = y_train[::10]\n",
    "        n_output_val = y_val.shape[1] * y_val.shape[2]\n",
    "        y_val = y_val.reshape((y_val.shape[0], n_output_val))\n",
    "        y_train = y_train[np.mod(np.arange(y_train.shape[0]),10)!=0]\n",
    "        val = ([orig_val, moving_average_val], y_val)\n",
    "    else:\n",
    "        val=None\n",
    "        orig, y_train= split_sequence(train, window, stepsout)\n",
    "        moving_average = split_sequences_multi(moving_frame(train, average_window), window, stepsout)\n",
    "        moving_average = moving_average.reshape(moving_average.shape[0], moving_average.shape[1], \n",
    "                                                moving_average.shape[2])\n",
    "    \n",
    "    if iteration == 0:\n",
    "        keras.backend.clear_session() \n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=l_rate, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    if optimizer == 'sgd':\n",
    "        opt = keras.optimizers.SGD(learning_rate=l_rate, momentum=0.9, nesterov=True)\n",
    "    if callback:\n",
    "        earlystop = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)]\n",
    "    else:\n",
    "        earlystop = None\n",
    "    \n",
    "    if iteration == 0:\n",
    "        n_output = y_train.shape[1] * y_train.shape[2]\n",
    "        y_train = y_train.reshape((y_train.shape[0], n_output))\n",
    "\n",
    "        input_raw = Input(batch_shape=(batch_size, orig.shape[1],orig.shape[2]))\n",
    "        input_smooth = Input(batch_shape=(batch_size, moving_average.shape[1], moving_average.shape[2]))\n",
    "\n",
    "        raw_branch = model_branch(orig.shape, filter_size, dense_neuron=neurons[0], batch_size=batch_size)\n",
    "        smooth_branch = model_branch(moving_average.shape, filter_size, dense_neuron=neurons[0], \n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "        raw_embedding = raw_branch(input_raw)\n",
    "        smooth_embedding = smooth_branch(input_smooth)\n",
    "\n",
    "        merged = concatenate([raw_embedding, smooth_embedding])\n",
    "    #     cnn_model = model_cnn(merged.get_shape()[1])\n",
    "    #     cnn_model = TimeDistributed(cnn_model)\n",
    "    #     cnn = cnn_model(merged)\n",
    "    #     cnn = cnn_model(merged)\n",
    "    #     shp1 = Reshape((1,cnn.shape[1]))(cnn)\n",
    "        skip00 = concatenate([input_raw, input_smooth, merged])\n",
    "        skip00 = concatenate([skip00, merged])\n",
    "        dropout00 = Dropout(drop)(skip00)\n",
    "        lstm1 = LSTM(neurons[1], return_sequences=True, stateful=True)(dropout00)\n",
    "        lstm2 = LSTM(neurons[1], return_sequences=True, stateful=True)(lstm1)\n",
    "        skip0 = concatenate([lstm2, merged])\n",
    "        dropout0 = Dropout(drop)(skip0)\n",
    "        dense1 = Dense(neurons[2], activation='relu')(dropout0)\n",
    "        dense2 = Dense(neurons[2], activation='relu')(dense1)\n",
    "        dense3 = Dense(neurons[2], activation='relu')(dense2)\n",
    "        skip1 = concatenate([dense3, lstm2])\n",
    "        dropout1 = Dropout(drop)(skip1)\n",
    "        dense4 = Dense(neurons[2]*.75, activation='relu')(dropout1)\n",
    "        dense5 = Dense(neurons[2]*.75, activation='relu')(dense4)\n",
    "        dense6 = Dense(neurons[2]*.75, activation='relu')(dense5) \n",
    "        skip2 = concatenate([dense6, skip1])\n",
    "        dropout2 = Dropout(drop)(skip2)\n",
    "        dense7 = Dense(neurons[2]*.75*.75, activation='relu')(dropout2)\n",
    "        dense8 = Dense(neurons[2]*.75*.75, activation='relu')(dense7)\n",
    "        dense9 = Dense(neurons[2]*.75*.75, activation='relu')(dense8)    \n",
    "        skip3 = concatenate([dense9, skip2])\n",
    "        dropout = Dropout(drop)(skip3)\n",
    "        output = Dense(n_output)(dropout)\n",
    "        model = Model(inputs=[input_raw, input_smooth], outputs=output)\n",
    "        model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "    if validate:\n",
    "        model_loss = model.fit([orig, moving_average], y_train, validation_data=val,\n",
    "                               epochs=nb_epoch, batch_size=batch_size, verbose=2, shuffle=False,\n",
    "                               callbacks=earlystop)\n",
    "        return model, model_loss.history['loss'], model_loss.history['val_loss']\n",
    "    else:\n",
    "        model_loss = model.fit([orig, moving_average], y_train, validation_data=val,\n",
    "                                epochs=nb_epoch, batch_size=batch_size, verbose=2, shuffle=False,\n",
    "                                callbacks=earlystop)\n",
    "        return model, model_loss.history['loss'], None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(1, 24, 1)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(1, 24, 3)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (1, 24, 10)          50          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (1, 24, 10)          130         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (1, 24, 20)          0           model[1][0]                      \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (1, 24, 24)          0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (1, 24, 44)          0           concatenate_1[0][0]              \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (1, 24, 44)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (1, 24, 600)         1548000     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (1, 24, 600)         2882400     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (1, 24, 620)         0           lstm_1[0][0]                     \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (1, 24, 620)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (1, 24, 600)         372600      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (1, 24, 600)         360600      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (1, 24, 600)         360600      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (1, 24, 1200)        0           dense_2[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (1, 24, 1200)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (1, 24, 450)         540450      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (1, 24, 450)         202950      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1, 24, 450)         202950      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (1, 24, 1650)        0           dense_5[0][0]                    \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (1, 24, 1650)        0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (1, 24, 337)         556387      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (1, 24, 337)         113906      dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (1, 24, 337)         113906      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (1, 24, 1987)        0           dense_8[0][0]                    \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (1, 24, 1987)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (1, 24, 12)          23856       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,278,785\n",
      "Trainable params: 7,278,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 425 samples, validate on 48 samples\n",
      "Epoch 1/25000\n",
      "425/425 - 9s - loss: 0.1317 - val_loss: 0.1260\n",
      "Epoch 2/25000\n",
      "425/425 - 5s - loss: 0.1284 - val_loss: 0.1244\n",
      "Epoch 3/25000\n",
      "425/425 - 5s - loss: 0.1274 - val_loss: 0.1239\n",
      "Epoch 4/25000\n",
      "425/425 - 5s - loss: 0.1270 - val_loss: 0.1237\n",
      "Epoch 5/25000\n",
      "425/425 - 5s - loss: 0.1269 - val_loss: 0.1236\n",
      "Epoch 6/25000\n",
      "425/425 - 5s - loss: 0.1269 - val_loss: 0.1235\n",
      "Epoch 7/25000\n",
      "425/425 - 5s - loss: 0.1268 - val_loss: 0.1235\n",
      "Epoch 8/25000\n",
      "425/425 - 5s - loss: 0.1268 - val_loss: 0.1234\n",
      "Epoch 9/25000\n",
      "425/425 - 5s - loss: 0.1268 - val_loss: 0.1234\n",
      "Epoch 10/25000\n",
      "425/425 - 5s - loss: 0.1268 - val_loss: 0.1234\n",
      "Epoch 11/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1234\n",
      "Epoch 12/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1234\n",
      "Epoch 13/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1234\n",
      "Epoch 14/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 15/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 16/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 17/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 18/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 19/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 20/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 21/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 22/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 23/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 24/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 25/25000\n",
      "425/425 - 5s - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 26/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 27/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 28/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 29/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 30/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 31/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 32/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 33/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 34/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 35/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 36/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 37/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 38/25000\n",
      "425/425 - 5s - loss: 0.1266 - val_loss: 0.1232\n",
      "Epoch 39/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 40/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 41/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 42/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 43/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 44/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 45/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 46/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 47/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 48/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 49/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 50/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 51/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 52/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 53/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1233\n",
      "Epoch 54/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 55/25000\n",
      "425/425 - 5s - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 56/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 57/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 58/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 59/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 60/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 61/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 62/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 63/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 64/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 65/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 66/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 67/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 68/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 69/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 70/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 71/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 72/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 73/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1233\n",
      "Epoch 74/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 75/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 76/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 77/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 78/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 79/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 80/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 81/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 82/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 83/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 84/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 85/25000\n",
      "425/425 - 5s - loss: 0.1264 - val_loss: 0.1232\n",
      "Epoch 86/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 87/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 88/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 89/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 90/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 91/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 92/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 93/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 94/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 95/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 96/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1233\n",
      "Epoch 97/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 98/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1233\n",
      "Epoch 99/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 100/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1232\n",
      "Epoch 101/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1232\n",
      "Epoch 102/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 103/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1233\n",
      "Epoch 104/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1233\n",
      "Epoch 105/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1232\n",
      "Epoch 106/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 107/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 108/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 109/25000\n",
      "425/425 - 5s - loss: 0.1263 - val_loss: 0.1233\n",
      "Epoch 110/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 111/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 112/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 113/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 114/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 115/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 116/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 117/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 118/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 119/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 120/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 121/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 122/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 123/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 124/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 125/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 126/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 127/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 128/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 129/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 130/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 131/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 132/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 133/25000\n",
      "425/425 - 5s - loss: 0.1261 - val_loss: 0.1233\n",
      "Epoch 134/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 135/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 136/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 137/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 138/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 139/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 140/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 141/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 142/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1234\n",
      "Epoch 143/25000\n",
      "425/425 - 5s - loss: 0.1261 - val_loss: 0.1233\n",
      "Epoch 144/25000\n",
      "425/425 - 5s - loss: 0.1261 - val_loss: 0.1233\n",
      "Epoch 145/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 146/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n",
      "Epoch 147/25000\n",
      "425/425 - 5s - loss: 0.1261 - val_loss: 0.1233\n",
      "Epoch 148/25000\n",
      "425/425 - 5s - loss: 0.1261 - val_loss: 0.1233\n",
      "Epoch 149/25000\n",
      "425/425 - 5s - loss: 0.1262 - val_loss: 0.1233\n"
     ]
    }
   ],
   "source": [
    "# transform data to be stationary\n",
    "raw_values = data.GS10.values\n",
    "diff_values = difference(raw_values, 1)\n",
    " \n",
    "# transform data to be supervised learning\n",
    "# supervised = timeseries_to_supervised(diff_values, 1)\n",
    "# print(supervised.shape)\n",
    "# supervised_values = supervised.values\n",
    " \n",
    "# split data into train and test-sets\n",
    "train, test = split_dataset_1(diff_values, .7)\n",
    " \n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train.values.reshape(-1, 1), test.values.reshape(-1, 1))\n",
    "\n",
    "window=24\n",
    "stepsout=12\n",
    "# fit the model\n",
    "time_start = datetime.now()\n",
    "conv_model, loss, val_loss = fit_convlstm(train_scaled, validate=True, batch_size=1, nb_epoch=25000,\n",
    "                                          neurons=(600,600,600), filter_size=4, drop=0.4, optimizer='sgd',\n",
    "                                          window=window, callback=True, stepsout=stepsout, l_rate=.0001,\n",
    "                                          patience=100)\n",
    "time_fit = datetime.now() - time_start\n",
    "\n",
    "# # forecast the entire training dataset to build up state for forecasting\n",
    "# # train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "\n",
    "# orig_test = test[:, 0].reshape(test.shape[0], 1, 1)\n",
    "# moving_average_test = moving_frame(test[:, 0], [3, 6, 12])\n",
    "# moving_average_test = moving_average_test.reshape(moving_average_test.shape[0], moving_average_test.shape[1], 1)\n",
    "# predictions = conv_model.predict([orig_test, moving_average_test], batch_size=None)\n",
    "loss += [np.nan] * (25000 - len(loss))\n",
    "val_loss += [np.nan] * (25000 - len(val_loss))\n",
    "loss_df = pd.DataFrame({'loss': loss})\n",
    "val_loss_df = pd.DataFrame({'val_loss': val_loss})\n",
    "\n",
    "start_idx = window + stepsout\n",
    "\n",
    "# walk-forward validation on the test data\n",
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "history = train.values[:start_idx]\n",
    "for i in range(len(train[start_idx:])):\n",
    "    scaler, train_scaled, test_scaled = scale(history.reshape(-1, 1), train[i+start_idx].reshape(1,-1))\n",
    "    # make one-step forecast\n",
    "    train_stack = np.vstack((train_scaled, test_scaled))\n",
    "    split = split_sequence(train_stack, window, stepsout)\n",
    "    X = split[0][-1]\n",
    "    y = split[1][-1]\n",
    "    orig_ = X.reshape(1, X.shape[0], X.shape[1])\n",
    "    moving_average_ = split_sequences_multi(moving_frame(train_stack, [3, 6, 12]), window, stepsout)[-1]\n",
    "    moving_average_ = moving_average_.reshape(1, moving_average_.shape[0], \n",
    "                                            moving_average_.shape[1])\n",
    "    yhat = conv_model.predict([orig_, moving_average_], batch_size=1)\n",
    "    yhat = yhat[-1]\n",
    "#     print(yhat.shape)\n",
    "    # invert scaling\n",
    "    yhat = invert_scale(scaler, X, yhat, stepsout)\n",
    "    # invert differencing\n",
    "    yhat = inverse_difference(raw_values, yhat, interval=len(train[start_idx:])+len(test)+1-i)\n",
    "    # store forecast\n",
    "    predictions.append(yhat)\n",
    "    history = np.append(history, train[i+start_idx])\n",
    "    \n",
    "history = train.values\n",
    "iteration = 0\n",
    "for i in range(len(test)):\n",
    "    scaler, train_scaled, test_scaled = scale(history.reshape(-1, 1), test.values[i].reshape(1, -1))\n",
    "    # make one-step forecast\n",
    "    train_stack = np.vstack((train_scaled, test_scaled))\n",
    "    split = split_sequence(train_stack, window, stepsout)\n",
    "    X = split[0][-1]\n",
    "    y = split[1][-1]\n",
    "    orig_ = X.reshape(1, X.shape[0], X.shape[1])\n",
    "    moving_average_ = split_sequences_multi(moving_frame(train_stack, [3, 6, 12]), window, stepsout)[-1]\n",
    "    moving_average_ = moving_average_.reshape(1, moving_average_.shape[0], \n",
    "                                            moving_average_.shape[1])\n",
    "    yhat = conv_model.predict([orig_, moving_average_], batch_size=1)\n",
    "    yhat = yhat[-1]\n",
    "    # invert scaling\n",
    "    yhat = invert_scale(scaler, X, yhat, stepsout)\n",
    "    # invert differencing\n",
    "    yhat = inverse_difference(raw_values, yhat, interval=len(test)+1-i)\n",
    "    # store forecast\n",
    "    predictions.append(yhat)\n",
    "    history = np.append(history, test.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 0:12:01.045794\n",
      "https://plot.ly/~acertainKnight/31/\n",
      "   Overall MAE  Overall MSE Period  Test MAE  Test MSE\n",
      "0     0.019951     0.002875    T+1  0.006647  0.000058\n",
      "0     0.023344     0.003715   T+10  0.005885  0.000044\n",
      "0     0.019692     0.002862   T+11  0.005528  0.000042\n",
      "0     0.022058     0.002880   T+12  0.007534  0.000079\n",
      "0     0.020805     0.002700    T+2  0.006363  0.000066\n",
      "0     0.020531     0.002603    T+3  0.005390  0.000051\n",
      "0     0.018355     0.002831    T+4  0.002452  0.000011\n",
      "0     0.021670     0.003366    T+5  0.006591  0.000051\n",
      "0     0.020945     0.002988    T+6  0.005556  0.000059\n",
      "0     0.020465     0.003142    T+7  0.006037  0.000048\n",
      "0     0.019331     0.003199    T+8  0.003601  0.000019\n",
      "0     0.019634     0.002734    T+9  0.005582  0.000042\n",
      "Overall MAE    0.020565\n",
      "Overall MSE    0.002991\n",
      "Test MAE       0.005597\n",
      "Test MSE       0.000047\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_46.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Time to train:', time_fit)\n",
    "predictions_array = np.array(predictions)\n",
    "# data_plt = pd.DataFrame(data.CPIAUCSL['2001-08-01':])\n",
    "pred_df = pd.DataFrame({'T+1': predictions_array[:, 0], 'T+2': predictions_array[:, 1],\n",
    "                         'T+3': predictions_array[:, 2], 'T+4': predictions_array[:, 3],\n",
    "                         'T+5': predictions_array[:, 4], 'T+6': predictions_array[:, 5],\n",
    "                         'T+7': predictions_array[:, 6], 'T+8': predictions_array[:, 7],\n",
    "                         'T+9': predictions_array[:, 8], 'T+10': predictions_array[:, 9],\n",
    "                         'T+11': predictions_array[:, 10], 'T+12': predictions_array[:, 11]})\n",
    "\n",
    "pred_plt = pd.DataFrame({'T+1' : pred_df['T+1'], 'T+2' : pred_df['T+2'].shift(1),\n",
    "                         'T+3' : pred_df['T+3'].shift(2), 'T+4' : pred_df['T+4'].shift(3),\n",
    "                         'T+5' : pred_df['T+5'].shift(4), 'T+6' : pred_df['T+6'].shift(5),\n",
    "                         'T+7' : pred_df['T+7'].shift(6), 'T+8' : pred_df['T+8'].shift(7),\n",
    "                         'T+9' : pred_df['T+9'].shift(8), 'T+10' : pred_df['T+10'].shift(9),\n",
    "                         'T+11': pred_df['T+11'].shift(10), 'T+12' : pred_df['T+12'].shift(11)})\n",
    "# for col in list(pred_plt):\n",
    "#     pred_plt['{}_cpi'.format(col)] = ((pred_plt[col].shift(-12) / pred_plt[col]) - 1)*100\n",
    "# for col in list(pred_plt)[:-12]:\n",
    "#     pred_plt['{}_cpi_diff'.format(col)] = pred_plt[col].shift(-1) - pred_plt[col]\n",
    "\n",
    "pred_plt['raw_values'] = raw_values[start_idx:-1]\n",
    "pred_plt['raw_'] = data.GS10.values[start_idx:-1]\n",
    "pred_plt['SPF T+1'] = data.SPFfor_Step1.values[start_idx:-1]\n",
    "pred_plt['SPF T+3'] = data.SPFfor_Step2.values[start_idx:-1]\n",
    "pred_plt['SPF T+6'] = data.SPFfor_Step3.values[start_idx:-1]\n",
    "pred_plt['SPF T+9'] = data.SPFfor_Step4.values[start_idx:-1]\n",
    "pred_plt['SPF T+12'] = data.SPFfor_Step5.values[start_idx:-1]\n",
    "\n",
    "pred_plt['DARM T+1'] = data.DARMfor_Step1.values[start_idx:-1]\n",
    "pred_plt['DARM T+3'] = data.DARMfor_Step2.values[start_idx:-1]\n",
    "pred_plt['DARM T+6'] = data.DARMfor_Step3.values[start_idx:-1]\n",
    "pred_plt['DARM T+9'] = data.DARMfor_Step4.values[start_idx:-1]\n",
    "pred_plt['DARM T+12'] = data.DARMfor_Step5.values[start_idx:-1]\n",
    "\n",
    "cols = ['SPF T+1','SPF T+3','SPF T+6','SPF T+9','SPF T+12', \n",
    "        'DARM T+1','DARM T+3','DARM T+6','DARM T+9','DARM T+12']\n",
    "pred_plt[cols] = pred_plt[cols].replace({0:np.nan})\n",
    "\n",
    "# pred_plt['raw_cpi_diff'] = pred_plt['raw_values'].shift(-1) - pred_plt['raw_values']\n",
    "pred_plt['date'] = data.index[start_idx:-1]\n",
    "pred_plt.set_index('date', inplace=True)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (24,16)\n",
    "\n",
    "results = pd.DataFrame({'Period':[], 'Overall MSE':[], 'Overall MAE':[], 'Test MSE':[], 'Test MAE':[]})\n",
    "\n",
    "for col in list(pred_plt)[:-12]:\n",
    "    ovr_mse = mean_squared_error(pred_plt['raw_'].values[11:-23], pred_plt[col].values[11:-23])\n",
    "    ovr_mae = mean_absolute_error(pred_plt['raw_'].values[11:-23], pred_plt[col].values[11:-23])\n",
    "    test_mse = mean_squared_error(pred_plt['raw_'].iloc[round((len(pred_plt)-20)*.7):-23], \n",
    "                                  pred_plt[col].iloc[round((len(pred_plt)-20)*.7):-23])\n",
    "    test_mae = mean_absolute_error(pred_plt['raw_'].iloc[round((len(pred_plt)-20)*.7):-23], \n",
    "                                   pred_plt[col].iloc[round((len(pred_plt)-20)*.7):-23])\n",
    "#     print(col,': Overall RMSE: %.3f' % sqrt(ovr_mse), 'Test RMSE: %.3f' % sqrt(test_mse))\n",
    "#     print(col,': Overall MAE: %.3f' % ovr_mae, 'Test MAE: %.3f' % test_mae)\n",
    "    row = pd.DataFrame({'Period':[col], 'Overall MSE':[ovr_mse], 'Overall MAE':[ovr_mae], 'Test MSE':[test_mse], 'Test MAE':[test_mae]})\n",
    "    results = results.append(row)\n",
    "    \n",
    "# for col in list(pred_plt)[:-2]:\n",
    "#     plt.plot(pred_plt[col][:-23], label=col)\n",
    "# # line plot of observed vs predicted\n",
    "# # plt.plot(raw_values[20:], color='blue')\n",
    "# plt.plot(pred_plt['raw_'][:-23], color='green', label='RAW')\n",
    "# # plt.plot(test[:,0], color='green')\n",
    "# plt.legend(prop={'size': 20})\n",
    "# plt.show()\n",
    "\n",
    "# for col in list(pred_plt)[:-2]:\n",
    "#     plt.plot(pred_plt[col].iloc[round((len(pred_plt)-20)*.7):-23], label=col)\n",
    "# plt.plot(pred_plt['raw_'].iloc[round((len(pred_plt)-20)*.7):-23], color='green', label='RAW')\n",
    "# # plt.plot(test[:,0], color='green')\n",
    "# plt.legend(prop={'size': 20})\n",
    "# plt.show()\n",
    "\n",
    "# i = 0\n",
    "# for col in list(pred_plt)[:-2]:\n",
    "#     plt.plot(pred_plt[col].iloc[round((len(pred_plt)-20)*.7):-23], label=col)\n",
    "#     plt.plot(pred_plt['raw_'].iloc[round((len(pred_plt)-20)*.7):-23], color='green', label='RAW')\n",
    "#     plt.title(col)\n",
    "#     plt.show()\n",
    "\n",
    "colorlist = ['#27ACAA', '#A4E9B9', '#228941', \n",
    "             '#9EE9E8', '#1D817F', '#A97FF0',\n",
    "             '#2E0C68', '#FFB3AF', '#9B0800',\n",
    "             '#FF6D00', '#BF5200', '#0C4672',\n",
    "             '#057ED8']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.update_layout(\n",
    "    paper_bgcolor='#FFFFFF',\n",
    "    plot_bgcolor='#FFFFFF'\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "        go.Scatter(x=list(pred_plt.index),\n",
    "                   y=list(pred_plt['raw_']),\n",
    "                   name='10-year rate',\n",
    "                   line=dict(color=colorlist[0])),)\n",
    "\n",
    "# Add Traces\n",
    "i = 0\n",
    "for col in ['T+1','T+2','T+3','T+4','T+5','T+6','T+7','T+8','T+9','T+10','T+11','T+12']:\n",
    "    i+=1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(pred_plt.index),\n",
    "                   y=list(pred_plt[col]),\n",
    "                   name='Model '+col,\n",
    "                   line=dict(color=colorlist[i], dash='dash')))\n",
    "    \n",
    "for col in cols:\n",
    "    i+=1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(pred_plt.index),\n",
    "                   y=list(pred_plt[col]),\n",
    "                   name=col,\n",
    "                   line=dict(dash='dot')))\n",
    "    \n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=go.layout.XAxis(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=218,\n",
    "                     label=\"Test period\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible=True\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        go.layout.Updatemenu(\n",
    "            active=0,\n",
    "            buttons=list([\n",
    "                dict(label=\"T+1\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, True, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, True, False, False,\n",
    "                                       False, False, True, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - One month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+2\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, True, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Two month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+3\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, True,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, True, False,\n",
    "                                       False, False, False, True, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Three month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+4\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Four month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+5\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, True, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Five month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+6\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, True, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, True,\n",
    "                                       False, False, False, False, True,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Six month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+7\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, True,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Seven month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+8\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Eight month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+9\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, True, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       True, False, False, False, False,\n",
    "                                       True, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Nine month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+10\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, True, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Ten month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+11\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, True,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate - Eleven month\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"T+12\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       True, False, False, False,\n",
    "                                       False, True, False, False, False,\n",
    "                                       False, True]},\n",
    "                           {\"title\": \"10-year Bond Rate - One year\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"All Forecasts\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, True, True, True,\n",
    "                                       True, True, True, True,\n",
    "                                       True, True, True, True,\n",
    "                                       True, False, False, False,\n",
    "                                       False, False, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate Model Forecasts\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"All SPF\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, True, True, True,\n",
    "                                       True, True, False, False, False,\n",
    "                                       False, False]},\n",
    "                           {\"title\": \"10-year Bond Rate SPF Forecasts\",\n",
    "                            \"annotations\": []}]),\n",
    "                dict(label=\"All DARM\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, False, False,\n",
    "                                       False, False, True, True, True,\n",
    "                                       True, True]},\n",
    "                           {\"title\": \"10-year Bond Rate DARM Forecasts\",\n",
    "                            \"annotations\": []}])\n",
    "            ]),\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 0, \"t\": 0},\n",
    "            showactive=True,\n",
    "            x=0,\n",
    "            xanchor=\"left\",\n",
    "            y=1,\n",
    "            yanchor=\"top\"\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "username = 'acertainKnight' # your username \n",
    "api_key = 'NPRtVstPcJfKUz8ZzM9P' # your api key - go to profile > settings > regenerate key \n",
    "chart_studio.tools.set_credentials_file(username=username, api_key=api_key)\n",
    "\n",
    "# first_plot_url = py.plot(fig, filename='10-year Bond Rate', auto_open=False,)\n",
    "print(first_plot_url)\n",
    "print(results)\n",
    "print(results.mean())\n",
    "fig.show(renderer='iframe')\n",
    "import plotly.io as pio\n",
    "pio.write_html(fig, file='index.html', auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_df['loss'], color='red', label='loss')\n",
    "plt.plot(val_loss_df['val_loss'], color='green', label='val_loss')\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
